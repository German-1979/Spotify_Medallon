{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77fe345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from extract import extract, load_bronze, transform_silver, aggregate_gold, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ebe5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import polars as pl\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7925680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Extract: descargar CSV a bronze\n",
    "# ------------------------------\n",
    "def extract(target_path):\n",
    "    \"\"\"\n",
    "    Descarga el dataset de Kaggle 'Ultimate Spotify Tracks DB' y copia\n",
    "    todos los archivos CSV a la carpeta 'bronze' dentro de target_path.\n",
    "    \n",
    "    Args:\n",
    "        target_path (str): Ruta donde se encuentra la carpeta 'data'.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Rutas completas de los CSV copiados en bronze.\n",
    "    \"\"\"\n",
    "    bronze_path = os.path.join(target_path, \"bronze\")\n",
    "    os.makedirs(bronze_path, exist_ok=True)\n",
    "    print(f\"[extract] Carpeta bronze lista en: {bronze_path}\")\n",
    "\n",
    "    cache_path = kagglehub.dataset_download(\"zaheenhamidani/ultimate-spotify-tracks-db\")\n",
    "    print(f\"[extract] Dataset descargado en cache: {cache_path}\")\n",
    "\n",
    "    csv_files = []\n",
    "    for file_name in os.listdir(cache_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            src = os.path.join(cache_path, file_name)\n",
    "            shutil.copy(src, bronze_path)\n",
    "            csv_files.append(os.path.join(bronze_path, file_name))\n",
    "            print(f\"[extract] Copiado a bronze: {file_name}\")\n",
    "\n",
    "    print(f\"[extract] CSV en bronze: {[os.path.basename(f) for f in csv_files]}\")\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3800b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2️⃣ Load Bronze: leer CSV con Polars + PyArrow\n",
    "# ------------------------------\n",
    "def load_bronze(csv_path):\n",
    "    \"\"\"\n",
    "    Lee un CSV desde bronze usando Polars con backend PyArrow.\n",
    "    \"\"\"\n",
    "    df_bronze = pl.read_csv(csv_path, use_pyarrow=True)\n",
    "    print(f\"[load_bronze] {len(df_bronze)} filas cargadas de {os.path.basename(csv_path)}\")\n",
    "    return df_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef10838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3️⃣ Transform Silver: limpieza y guardado incremental\n",
    "# ------------------------------\n",
    "def transform_silver(df_bronze, silver_path, csv_name):\n",
    "    \"\"\"\n",
    "    Limpia nulos y guarda el DataFrame en Silver como Parquet.\n",
    "    Si ya existe, lo carga directamente (incremental).\n",
    "    \"\"\"\n",
    "    os.makedirs(silver_path, exist_ok=True)\n",
    "    silver_file = os.path.join(silver_path, f\"{csv_name}_silver.parquet\")\n",
    "\n",
    "    if os.path.exists(silver_file):\n",
    "        print(f\"[transform_silver] Silver ya existe, cargando: {silver_file}\")\n",
    "        df_silver = pl.read_parquet(silver_file, engine=\"pyarrow\")\n",
    "    else:\n",
    "        # Rellenar nulos de forma vectorizada\n",
    "        fill_dict = {}\n",
    "        for col, dtype in zip(df_bronze.columns, df_bronze.dtypes):\n",
    "            if dtype == pl.Utf8:\n",
    "                fill_dict[col] = \"N/A\"\n",
    "            elif dtype in [pl.Int64, pl.Float64]:\n",
    "                fill_dict[col] = 0\n",
    "        df_silver = df_bronze.fill_null(fill_dict)\n",
    "\n",
    "        df_silver.write_parquet(silver_file, engine=\"pyarrow\")\n",
    "        print(f\"[transform_silver] Silver guardado en: {silver_file}\")\n",
    "\n",
    "    return df_silver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef89d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4️⃣ Aggregate Gold: agregación incremental\n",
    "# ------------------------------\n",
    "def aggregate_gold(df_silver, gold_path, csv_name):\n",
    "    \"\"\"\n",
    "    Agrega datos para la capa Gold y guarda como Parquet.\n",
    "    Solo procesa si no existe ya el Gold.\n",
    "    \"\"\"\n",
    "    os.makedirs(gold_path, exist_ok=True)\n",
    "    gold_file = os.path.join(gold_path, f\"{csv_name}_gold.parquet\")\n",
    "\n",
    "    if os.path.exists(gold_file):\n",
    "        print(f\"[aggregate_gold] Gold ya existe, cargando: {gold_file}\")\n",
    "        df_gold = pl.read_parquet(gold_file, engine=\"pyarrow\")\n",
    "    else:\n",
    "        if \"artist_name\" in df_silver.columns and \"track_name\" in df_silver.columns:\n",
    "            df_gold = df_silver.groupby(\"artist_name\").agg(\n",
    "                pl.count(\"track_name\").alias(\"track_count\")\n",
    "            ).sort(\"track_count\", reverse=True)\n",
    "        else:\n",
    "            df_gold = df_silver\n",
    "        df_gold.write_parquet(gold_file, engine=\"pyarrow\")\n",
    "        print(f\"[aggregate_gold] Gold guardado en: {gold_file}\")\n",
    "\n",
    "    return df_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dece1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5️⃣ Main pipeline: orquestador incremental\n",
    "# ------------------------------\n",
    "def main():\n",
    "    base_path = os.path.join(os.getcwd(), \"data\")\n",
    "    silver_path = os.path.join(base_path, \"silver\")\n",
    "    gold_path = os.path.join(base_path, \"gold\")\n",
    "\n",
    "    # 1️⃣ Extract\n",
    "    csv_files = extract(base_path)\n",
    "\n",
    "    gold_results = {}\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        csv_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "\n",
    "        # 2️⃣ Load Bronze\n",
    "        df_bronze = load_bronze(csv_path)\n",
    "\n",
    "        # 3️⃣ Transform Silver (incremental)\n",
    "        df_silver = transform_silver(df_bronze, silver_path, csv_name)\n",
    "\n",
    "        # 4️⃣ Aggregate Gold (incremental)\n",
    "        df_gold = aggregate_gold(df_silver, gold_path, csv_name)\n",
    "\n",
    "        gold_results[csv_name] = df_gold\n",
    "\n",
    "    print(\"\\n[main] Pipeline completado. Resultados Gold listos en memoria.\")\n",
    "    return gold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f86f15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extract] Carpeta bronze lista en: C:\\Users\\germa\\Desktop\\Carpetas\\Data_Engineer_Specialist\\Spotify_Medallon\\notebooks\\data\\bronze\n",
      "[extract] Dataset descargado en cache: C:\\Users\\germa\\.cache\\kagglehub\\datasets\\zaheenhamidani\\ultimate-spotify-tracks-db\\versions\\3\n",
      "[extract] Copiado a bronze: SpotifyFeatures.csv\n",
      "[extract] CSV en bronze: ['SpotifyFeatures.csv']\n",
      "[load_bronze] 232725 filas cargadas de SpotifyFeatures.csv\n"
     ]
    },
    {
     "ename": "InvalidOperationError",
     "evalue": "must specify one field in the struct",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidOperationError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ejecutar\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     gold_dict = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m df_bronze = load_bronze(csv_path)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 3️⃣ Transform Silver (incremental)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df_silver = \u001b[43mtransform_silver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_bronze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilver_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 4️⃣ Aggregate Gold (incremental)\u001b[39;00m\n\u001b[32m     24\u001b[39m df_gold = aggregate_gold(df_silver, gold_path, csv_name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtransform_silver\u001b[39m\u001b[34m(df_bronze, silver_path, csv_name)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m [pl.Int64, pl.Float64]:\n\u001b[32m     22\u001b[39m         fill_dict[col] = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m df_silver = \u001b[43mdf_bronze\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m df_silver.write_parquet(silver_file, engine=\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[transform_silver] Silver guardado en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilver_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Carpetas\\Data_Engineer_Specialist\\Spotify_Medallon\\venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:9130\u001b[39m, in \u001b[36mDataFrame.fill_null\u001b[39m\u001b[34m(self, value, strategy, limit, matches_supertype)\u001b[39m\n\u001b[32m   9037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9038\u001b[39m \u001b[33;03mFill null values using the specified value or strategy.\u001b[39;00m\n\u001b[32m   9039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   9123\u001b[39m \u001b[33;03m└─────┴──────┘\u001b[39;00m\n\u001b[32m   9124\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9125\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   9127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   9128\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9129\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches_supertype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatches_supertype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m9130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9131\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Carpetas\\Data_Engineer_Specialist\\Spotify_Medallon\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Carpetas\\Data_Engineer_Specialist\\Spotify_Medallon\\venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Carpetas\\Data_Engineer_Specialist\\Spotify_Medallon\\venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2422\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2420\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2421\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mInvalidOperationError\u001b[39m: must specify one field in the struct"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Ejecutar\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    gold_dict = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0bc37-6626-4238-97ab-2da13d55ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spotify_Medallon)",
   "language": "python",
   "name": "spotify_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
